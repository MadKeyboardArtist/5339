{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b112a6-a0c7-4151-b841-3195129d74cf",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431b1cd4-d7a4-4990-b872-948fa296f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geoalchemy2 in c:\\users\\zheng\\anaconda3\\lib\\site-packages (0.18.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: SQLAlchemy>=1.4 in c:\\users\\zheng\\anaconda3\\lib\\site-packages (from geoalchemy2) (2.0.43)\n",
      "Requirement already satisfied: packaging in c:\\users\\zheng\\anaconda3\\lib\\site-packages (from geoalchemy2) (23.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\zheng\\anaconda3\\lib\\site-packages (from SQLAlchemy>=1.4->geoalchemy2) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\zheng\\anaconda3\\lib\\site-packages (from SQLAlchemy>=1.4->geoalchemy2) (4.11.0)\n"
     ]
    }
   ],
   "source": [
    "pip install geoalchemy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a68d8596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "   ---------------------------------------- 0.0/250.9 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/250.9 kB 991.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 250.9/250.9 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3685f4f-48ef-4be4-96b7-a0765ae3127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import URL\n",
    "from geoalchemy2 import Geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5365c-cd1d-4ad6-b759-74884999772c",
   "metadata": {},
   "source": [
    "## abs single header\n",
    "Take Table 1 and clear the irrelevant headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ea3320-d126-4406-b589-5b41873fe66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "XLSX = \"14100DO0003_2011-24.xlsx\"  \n",
    "SHEET = \"Table 1\"\n",
    "OUT   = \"abs_table1_raw_single_header.csv\"\n",
    "\n",
    "raw = pd.read_excel(XLSX, sheet_name=SHEET, header=None)\n",
    "\n",
    "# find “Code / Label / Year...”\n",
    "header_row = None\n",
    "for i in range(len(raw)):\n",
    "    row = raw.iloc[i, :3].astype(str).str.strip().tolist()\n",
    "    if row[0].lower() == \"code\" and row[1].lower() == \"label\" and row[2].lower() == \"year\":\n",
    "        header_row = i\n",
    "        break\n",
    "\n",
    "# create new file\n",
    "df = pd.read_excel(XLSX, sheet_name=SHEET, header=header_row)\n",
    "df.to_csv(OUT, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c8e21-9df5-4b51-abb1-a213105f9501",
   "metadata": {},
   "source": [
    "## select rows of abs\n",
    "There is too much content in the abs table. We take out the key columns and carry them into the next analysis. In the report, we need to analyze why these columns were taken out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c8a9d6-c768-4177-9508-e395b3c6f27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> abs_table1_core_selected.csv | shape=(29097, 17)\n"
     ]
    }
   ],
   "source": [
    "SRC = \"abs_table1_raw_single_header.csv\"   \n",
    "OUT = \"abs_table1_core_selected.csv\"  \n",
    "\n",
    "# match exact name\n",
    "COLUMNS = [\n",
    "    \"Code\", \"Label\", \"Year\",\n",
    "    \"Total number of businesses\",\n",
    "    \"Number of non-employing businesses\",\n",
    "    \"Number of employing businesses: 1-4 employees\",\n",
    "    \"Number of employing businesses: 5-19 employees\",\n",
    "    \"Number of employing businesses: 20 or more employees\",\n",
    "    \"Mining (no.)\",\n",
    "    \"Manufacturing (no.)\",\n",
    "    \"Electricity, gas, water and waste services (no.)\",\n",
    "    \"Construction (no.)\",\n",
    "    \"Transport, postal and warehousing (no.)\",\n",
    "    \"Number of businesses with turnover of $200k to less than $2m\",\n",
    "    \"Number of businesses with turnover of $2m to less than $5m\",\n",
    "    \"Number of businesses with turnover of $5m to less than $10m\",\n",
    "    \"Number of businesses with turnover of $10m or more\",\n",
    "]\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(SRC, low_memory=False)\n",
    "    out = df[COLUMNS].copy()\n",
    "    out.to_csv(OUT, index=False)\n",
    "    print(f\"Saved -> {OUT} | shape={out.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a3b3f-bef0-4860-9fd9-33fdcd207792",
   "metadata": {},
   "source": [
    "We have cleaned up these columns, mainly keeping only a few rows of states and unifying the names of the states into abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88f25288-84aa-470a-9d7e-8eca965fe210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> ABS_table1_core_selected_8states_AUS.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the ABS table\n",
    "df = pd.read_csv(\"abs_table1_core_selected.csv\")\n",
    "\n",
    "# Drop the first column if it is just an index/sequence\n",
    "if df.columns[0].lower().startswith(\"unnamed\"):\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "# State name mapping\n",
    "state_map = {\n",
    "    \"New South Wales\": \"NSW\",\n",
    "    \"Victoria\": \"VIC\",\n",
    "    \"Queensland\": \"QLD\",\n",
    "    \"South Australia\": \"SA\",\n",
    "    \"Western Australia\": \"WA\",\n",
    "    \"Tasmania\": \"TAS\",\n",
    "    \"Australian Capital Territory\": \"ACT\",\n",
    "    \"Northern Territory\": \"NT\",\n",
    "    \"Australia\": \"AUS\",\n",
    "    # If already an abbreviation, keep as is\n",
    "    \"NSW\": \"NSW\", \"VIC\": \"VIC\", \"QLD\": \"QLD\", \"SA\": \"SA\",\n",
    "    \"WA\": \"WA\", \"TAS\": \"TAS\", \"ACT\": \"ACT\", \"NT\": \"NT\", \"AUS\": \"AUS\"\n",
    "}\n",
    "\n",
    "# Create the State column: prefer Label, fallback to Code\n",
    "if \"Label\" in df.columns:\n",
    "    df[\"State\"] = df[\"Label\"].map(state_map).fillna(df[\"Code\"])\n",
    "elif \"Code\" in df.columns:\n",
    "    df[\"State\"] = df[\"Code\"].map(state_map).fillna(df[\"Code\"])\n",
    "else:\n",
    "    raise KeyError(\"No state name field found (Label or Code).\")\n",
    "\n",
    "# Keep only 8 states + AUS\n",
    "valid_states = {\"NSW\",\"VIC\",\"QLD\",\"SA\",\"WA\",\"TAS\",\"ACT\",\"NT\",\"AUS\"}\n",
    "df = df[df[\"State\"].isin(valid_states)]\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"ABS_table1_core_selected_8states_AUS.csv\", index=False)\n",
    "\n",
    "print(\"Saved -> ABS_table1_core_selected_8states_AUS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "257f6ea8-546d-4da4-abb8-48a9b4c4b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> ABS_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"ABS_table1_core_selected_8states_AUS.csv\")\n",
    "if \"Code\" in df.columns:\n",
    "    df = df[~((df[\"State\"]==\"ACT\") & (df[\"Code\"]!=\"8\"))]\n",
    "\n",
    "# 1) delete Code and Label （if exist）\n",
    "df = df.drop(columns=[c for c in [\"Code\",\"Label\"] if c in df.columns])\n",
    "\n",
    "\n",
    "# save\n",
    "df.to_csv(\"ABS_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"Saved -> ABS_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257936e-132c-488d-88f9-d606c8e156c0",
   "metadata": {},
   "source": [
    "### NGER data integration and weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b35cce-d5e9-4557-b8d8-0406729c8b40",
   "metadata": {},
   "source": [
    "Merge multiple tables and mark the year at the end of each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba70a68-c3aa-45a9-a316-32d67afa210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# File list (ordered by 2014–2023)\n",
    "files = [\n",
    "    \"NGER.ID0075.csv\",  # 2014\n",
    "    \"NGER.ID0076.csv\",  # 2015\n",
    "    \"NGER.ID0077.csv\",  # 2016\n",
    "    \"NGER.ID0078.csv\",  # 2017\n",
    "    \"NGER.ID0079.csv\",  # 2018\n",
    "    \"NGER.ID0080.csv\",  # 2019\n",
    "    \"NGER.ID0081.csv\",  # 2020\n",
    "    \"NGER.ID0082.csv\",  # 2021\n",
    "    \"NGER.ID0083.csv\",  # 2022\n",
    "    \"NGER.ID0243.csv\",  # 2023\n",
    "]\n",
    "\n",
    "# Generate years corresponding to each file\n",
    "years = list(range(2014, 2014 + len(files)))\n",
    "\n",
    "dfs = []\n",
    "for f, y in zip(files, years):\n",
    "    df = pd.read_csv(f, header=None)   # Do not use header, stack rows directly\n",
    "    df[\"year\"] = y                     # Add a year column at the end\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate in order\n",
    "nger_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save combined result\n",
    "nger_all.to_csv(\"NGER_2014_2023_all_rawconcat.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ce49f-1e45-4d8d-a980-afeeb2b6ed72",
   "metadata": {},
   "source": [
    "Convert the original concatenated data of NGER from 2014 to 2023 into a state-year summary table, including total power generation, total emissions and weighted emission intensity, and supplement the National total (AUS) row at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45a3176b-e424-4b5e-a540-060eef2c1770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - NGER_cleaned.csv\n",
      "  State  year  total_mwh  total_emissions  total_wx  \\\n",
      "0   ACT  2014    98106.0           1708.0   1509.84   \n",
      "1   ACT  2015    63256.0           1788.0   1503.46   \n",
      "2   ACT  2016    26750.0           1768.0   1872.50   \n",
      "3   ACT  2017    73003.0           2355.0   2048.59   \n",
      "4   ACT  2018    93555.0           2061.0   1867.46   \n",
      "5   ACT  2019    87670.0           1575.0   1400.16   \n",
      "6   ACT  2020    90253.0           2979.0   2802.60   \n",
      "7   ACT  2021    84427.0           3786.0   3258.70   \n",
      "8   ACT  2022   246566.0           5287.0   4152.00   \n",
      "9   ACT  2023   224556.0           5621.0   4278.24   \n",
      "\n",
      "   weighted_intensity_t_per_mwh  \n",
      "0                      0.015390  \n",
      "1                      0.023768  \n",
      "2                      0.070000  \n",
      "3                      0.028062  \n",
      "4                      0.019961  \n",
      "5                      0.015971  \n",
      "6                      0.031053  \n",
      "7                      0.038598  \n",
      "8                      0.016839  \n",
      "9                      0.019052  \n"
     ]
    }
   ],
   "source": [
    "SRC = \"NGER_2014_2023_all_rawconcat.csv\"\n",
    "\n",
    "COLS = [\n",
    "    \"Reporting Entity\",\"Facility Name\",\"Type\",\"State\",\n",
    "    \"Electricity Production GJ\",\"Electricity Production Mwh\",\n",
    "    \"Scope 1 t CO2 e\",\"Scope 2 t CO2 e\",\"Total Emissions t CO2 e\",\n",
    "    \"Emission Intensity t Mwh\",\"Grid Connected\",\"Grid\",\"Primary Fuel\",\n",
    "    \"Important Notes\",\"year\"\n",
    "]\n",
    "\n",
    "# ===== 1) Load raw concatenated file and assign column names =====\n",
    "df = pd.read_csv(SRC, header=None, low_memory=False)\n",
    "df.columns = COLS\n",
    "\n",
    "# ===== 2) Convert numeric columns =====\n",
    "num_cols = [\n",
    "    \"Electricity Production Mwh\",\n",
    "    \"Total Emissions t CO2 e\",\n",
    "    \"Emission Intensity t Mwh\",\n",
    "]\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# ===== 3) Aggregate by State-Year (totals + weighted intensity) =====\n",
    "tmp = df.dropna(subset=[\"Electricity Production Mwh\",\"Emission Intensity t Mwh\"]).copy()\n",
    "tmp[\"wx\"] = tmp[\"Electricity Production Mwh\"] * tmp[\"Emission Intensity t Mwh\"]\n",
    "\n",
    "state_year = tmp.groupby([\"State\",\"year\"], as_index=False).agg(\n",
    "    total_mwh=(\"Electricity Production Mwh\",\"sum\"),\n",
    "    total_emissions=(\"Total Emissions t CO2 e\",\"sum\"),\n",
    "    total_wx=(\"wx\",\"sum\"),\n",
    ")\n",
    "state_year[\"weighted_intensity_t_per_mwh\"] = state_year[\"total_wx\"] / state_year[\"total_mwh\"]\n",
    "\n",
    "# ===== 4) National totals (AUS) =====\n",
    "aus = state_year.groupby(\"year\", as_index=False).agg(\n",
    "    total_mwh=(\"total_mwh\",\"sum\"),\n",
    "    total_emissions=(\"total_emissions\",\"sum\"),\n",
    "    total_wx=(\"total_wx\",\"sum\"),\n",
    ")\n",
    "aus[\"weighted_intensity_t_per_mwh\"] = aus[\"total_wx\"] / aus[\"total_mwh\"]\n",
    "aus.insert(0, \"State\", \"AUS\")  # Add State column\n",
    "\n",
    "# ===== 5) Combine state-level and national-level results and save =====\n",
    "final = pd.concat([state_year, aus], ignore_index=True).sort_values([\"State\",\"year\"])\n",
    "final.to_csv(\"NGER_cleaned.csv\", index=False)\n",
    "\n",
    "print(\" - NGER_cleaned.csv\")\n",
    "print(final.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7040bac-bb65-4f5f-b018-b07cdfa9d75e",
   "metadata": {},
   "source": [
    "### clean cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d48a1da-6ef9-4813-8740-d5bb73c1fc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  State  year  accredited_capacity_mw  accredited_projects\n",
      "0   ACT  2015                  0.7760                    2\n",
      "1   ACT  2016                 27.4780                    6\n",
      "2   ACT  2018                  0.8428                    3\n",
      "3   ACT  2019                  0.5493                    2\n",
      "4   ACT  2020                  7.3472                    7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zheng\\AppData\\Local\\Temp\\ipykernel_24124\\2883078743.py:6: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  cer[\"year\"] = pd.to_datetime(cer[\"Accreditation start date\"], errors=\"coerce\").dt.year\n"
     ]
    }
   ],
   "source": [
    "cer = pd.read_csv(\"historical-accredited-power-stations-and-projects-0.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Cleaning\n",
    "cer[\"State\"] = cer[\"State\"].astype(str).str.strip().str.upper()\n",
    "cer[\"Installed capacity\"] = pd.to_numeric(cer[\"Installed capacity\"], errors=\"coerce\")\n",
    "cer[\"year\"] = pd.to_datetime(cer[\"Accreditation start date\"], errors=\"coerce\").dt.year\n",
    "\n",
    "# Keep only 2015–2024\n",
    "cer = cer[cer[\"year\"].between(2015, 2024)]\n",
    "\n",
    "# Aggregate by state + year\n",
    "summary = cer.groupby([\"State\",\"year\"], as_index=False).agg(\n",
    "    accredited_capacity_mw=(\"Installed capacity\",\"sum\"),\n",
    "    accredited_projects=(\"Installed capacity\",\"count\")\n",
    ")\n",
    "\n",
    "# Save cleaned table\n",
    "summary.to_csv(\"CER_cleaned.csv\", index=False)\n",
    "print(summary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ca56b-3dfd-4bfa-8d07-af904b6c5ec5",
   "metadata": {},
   "source": [
    "### Combine the retrieved data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009bbc22-e54d-4da5-8744-7e5628599f96",
   "metadata": {},
   "source": [
    "Clean and merge the data tables related to energy and industry from three different sources (CER, NGER, ABS) into a unified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed67113f-5fee-44c6-a73c-36ddfa96abe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  state  year  total_mwh  total_emissions  total_wx  \\\n",
      "2   ACT  2015    63256.0           1788.0   1503.46   \n",
      "3   ACT  2016    26750.0           1768.0   1872.50   \n",
      "4   ACT  2017    73003.0           2355.0   2048.59   \n",
      "5   ACT  2018    93555.0           2061.0   1867.46   \n",
      "6   ACT  2019    87670.0           1575.0   1400.16   \n",
      "\n",
      "   weighted_intensity_t_per_mwh  accredited_capacity_mw  accredited_projects  \\\n",
      "2                      0.023768                  0.7760                  2.0   \n",
      "3                      0.070000                 27.4780                  6.0   \n",
      "4                      0.028062                     NaN                  NaN   \n",
      "5                      0.019961                  0.8428                  3.0   \n",
      "6                      0.015971                  0.5493                  2.0   \n",
      "\n",
      "   total number of businesses  number of non-employing businesses  ...  \\\n",
      "2                         NaN                                 NaN  ...   \n",
      "3                         NaN                                 NaN  ...   \n",
      "4                         NaN                                 NaN  ...   \n",
      "5                         NaN                                 NaN  ...   \n",
      "6                         NaN                                 NaN  ...   \n",
      "\n",
      "   number of employing businesses: 20 or more employees  mining (no.)  \\\n",
      "2                                                NaN              NaN   \n",
      "3                                                NaN              NaN   \n",
      "4                                                NaN              NaN   \n",
      "5                                                NaN              NaN   \n",
      "6                                                NaN              NaN   \n",
      "\n",
      "   manufacturing (no.)  electricity, gas, water and waste services (no.)  \\\n",
      "2                  NaN                                               NaN   \n",
      "3                  NaN                                               NaN   \n",
      "4                  NaN                                               NaN   \n",
      "5                  NaN                                               NaN   \n",
      "6                  NaN                                               NaN   \n",
      "\n",
      "   construction (no.)  transport, postal and warehousing (no.)  \\\n",
      "2                 NaN                                      NaN   \n",
      "3                 NaN                                      NaN   \n",
      "4                 NaN                                      NaN   \n",
      "5                 NaN                                      NaN   \n",
      "6                 NaN                                      NaN   \n",
      "\n",
      "   number of businesses with turnover of $200k to less than $2m  \\\n",
      "2                                                NaN              \n",
      "3                                                NaN              \n",
      "4                                                NaN              \n",
      "5                                                NaN              \n",
      "6                                                NaN              \n",
      "\n",
      "   number of businesses with turnover of $2m to less than $5m  \\\n",
      "2                                                NaN            \n",
      "3                                                NaN            \n",
      "4                                                NaN            \n",
      "5                                                NaN            \n",
      "6                                                NaN            \n",
      "\n",
      "   number of businesses with turnover of $5m to less than $10m  \\\n",
      "2                                                NaN             \n",
      "3                                                NaN             \n",
      "4                                                NaN             \n",
      "5                                                NaN             \n",
      "6                                                NaN             \n",
      "\n",
      "   number of businesses with turnover of $10m or more  \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "5                                                NaN   \n",
      "6                                                NaN   \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define missing value markers\n",
    "missing_values = [\"-\", \"--\", \"<Null>\", \"null\", \"NULL\", \"\"]\n",
    "\n",
    "# 1) Read the three datasets and automatically treat missing values as NaN\n",
    "cer = pd.read_csv(\"CER_cleaned.csv\", na_values=missing_values)\n",
    "nger = pd.read_csv(\"NGER_cleaned.csv\", na_values=missing_values)\n",
    "absd = pd.read_csv(\"ABS_cleaned.csv\", na_values=missing_values)\n",
    "\n",
    "# 2) Function: normalize column names (all lowercase, strip spaces)\n",
    "def normalize_cols(df):\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "cer = normalize_cols(cer)\n",
    "nger = normalize_cols(nger)\n",
    "absd = normalize_cols(absd)\n",
    "\n",
    "# 3) Ensure 'state' and 'year' columns exist and standardize them\n",
    "for df in [cer, nger, absd]:\n",
    "    if \"state\" not in df.columns:\n",
    "        raise KeyError(\"Missing 'state' column\")\n",
    "    if \"year\" not in df.columns:\n",
    "        raise KeyError(\"Missing 'year' column\")\n",
    "    df[\"state\"] = df[\"state\"].astype(str).str.strip().str.upper()\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# 4) Merge NGER + CER + ABS on state and year\n",
    "merged = pd.merge(nger, cer, on=[\"state\", \"year\"], how=\"outer\")\n",
    "merged = pd.merge(merged, absd, on=[\"state\", \"year\"], how=\"outer\")\n",
    "\n",
    "# 5) Drop invalid rows (where state or year is missing)\n",
    "merged = merged.dropna(subset=[\"state\", \"year\"], how=\"any\")\n",
    "\n",
    "# 6) Keep only rows from 2015 onwards\n",
    "merged = merged[merged[\"year\"] >= 2015]\n",
    "\n",
    "# 7) Save the final merged dataset\n",
    "merged.to_csv(\"ABS_NGER_CER_merged.csv\", index=False)\n",
    "\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c1eac9e-bfb6-49a1-81d7-bd707bf53c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q googlemaps \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9ba57f1-56b4-4e61-b251-5f0ec913ec34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accreditation code</th>\n",
       "      <th>Power station name</th>\n",
       "      <th>State</th>\n",
       "      <th>Installed capacity</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Fuel source(s)</th>\n",
       "      <th>Accreditation start date</th>\n",
       "      <th>Suspension status</th>\n",
       "      <th>Baseline (MWh)</th>\n",
       "      <th>Comment</th>\n",
       "      <th>_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SRPXQLG5</td>\n",
       "      <td>Varsity Views - Solar w SGU - QLD</td>\n",
       "      <td>QLD</td>\n",
       "      <td>0.2109</td>\n",
       "      <td>4227</td>\n",
       "      <td>Solar</td>\n",
       "      <td>18/12/2024</td>\n",
       "      <td>Unsuspended</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Varsity Views - Solar w SGU - QLD, QLD, 4227, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SRPYNS51</td>\n",
       "      <td>NNSWLHD-Byron Central Hospital - Solar - NSW</td>\n",
       "      <td>NSW</td>\n",
       "      <td>0.7203</td>\n",
       "      <td>2481</td>\n",
       "      <td>Solar</td>\n",
       "      <td>12/12/2024</td>\n",
       "      <td>Unsuspended</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNSWLHD-Byron Central Hospital - Solar - NSW, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRPXQLG2</td>\n",
       "      <td>Springwood Terrace Care Community - Solar w SG...</td>\n",
       "      <td>QLD</td>\n",
       "      <td>0.1890</td>\n",
       "      <td>4127</td>\n",
       "      <td>Solar</td>\n",
       "      <td>10/12/2024</td>\n",
       "      <td>Unsuspended</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Springwood Terrace Care Community - Solar w SG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Accreditation code                                 Power station name State  \\\n",
       "0           SRPXQLG5                  Varsity Views - Solar w SGU - QLD   QLD   \n",
       "1           SRPYNS51       NNSWLHD-Byron Central Hospital - Solar - NSW   NSW   \n",
       "2           SRPXQLG2  Springwood Terrace Care Community - Solar w SG...   QLD   \n",
       "\n",
       "   Installed capacity  Postcode Fuel source(s) Accreditation start date  \\\n",
       "0              0.2109      4227          Solar               18/12/2024   \n",
       "1              0.7203      2481          Solar               12/12/2024   \n",
       "2              0.1890      4127          Solar               10/12/2024   \n",
       "\n",
       "  Suspension status Baseline (MWh) Comment  \\\n",
       "0       Unsuspended              0     NaN   \n",
       "1       Unsuspended              0     NaN   \n",
       "2       Unsuspended              0     NaN   \n",
       "\n",
       "                                              _query  \n",
       "0  Varsity Views - Solar w SGU - QLD, QLD, 4227, ...  \n",
       "1  NNSWLHD-Byron Central Hospital - Solar - NSW, ...  \n",
       "2  Springwood Terrace Care Community - Solar w SG...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import googlemaps\n",
    "\n",
    "IN_CSV  = Path(\"historical-accredited-power-stations-and-projects-0.csv\")  # 按需修改\n",
    "OUT_CSV = Path(\"stations_with_geo.csv\")\n",
    "CACHE_CSV = Path(\"geocode_cache.csv\")\n",
    "\n",
    "# 读取（兼容编码问题）\n",
    "try:\n",
    "    df = pd.read_csv(IN_CSV)\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(IN_CSV, encoding=\"ISO-8859-1\")\n",
    "\n",
    "COL_NAME, COL_STATE, COL_POSTCODE = \"Power station name\", \"State\", \"Postcode\"\n",
    "\n",
    "def sanitize(x):\n",
    "    return \"\" if pd.isna(x) else str(x).strip()\n",
    "\n",
    "def make_query(row):\n",
    "    # 用“电站名 + 州 + 邮编 + Australia”，并加上区域偏置\n",
    "    return \", \".join([p for p in [\n",
    "        sanitize(row[COL_NAME]),\n",
    "        sanitize(row[COL_STATE]),\n",
    "        sanitize(row[COL_POSTCODE]),\n",
    "        \"Australia\"\n",
    "    ] if p])\n",
    "\n",
    "df[\"_query\"] = df.apply(make_query, axis=1)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de3ed628-f6dd-4772-ba1e-02261edb7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyBRf4lXjnQsjfBQ3BusqBBJ8FK7iPOxFWM\"\n",
    "\n",
    "gmaps = googlemaps.Client(key=API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "144e7108-635d-4de8-9a99-1c76fd204a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_once(query):\n",
    "    try:\n",
    "        res = gmaps.geocode(query, region=\"au\")\n",
    "        if not res:\n",
    "            return None, None, \"ZERO_RESULTS\", \"\"\n",
    "        top = res[0]\n",
    "        loc = top[\"geometry\"][\"location\"]\n",
    "        return loc[\"lat\"], loc[\"lng\"], \"OK\", top.get(\"formatted_address\", \"\")\n",
    "    except Exception as e:\n",
    "        return None, None, f\"ERROR:{type(e).__name__}\", \"\"\n",
    "\n",
    "def geocode_with_backoff(query, retries=5, base=1.0):\n",
    "    for k in range(retries):\n",
    "        lat, lng, status, fmt = geocode_once(query)\n",
    "        if status in (\"OK\", \"ZERO_RESULTS\"):\n",
    "            return lat, lng, status, fmt\n",
    "        time.sleep(base * (2 ** k))  # 退避等待\n",
    "    return None, None, status, fmt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27aef2-5f39-4e8f-8717-3e5de0a36f93",
   "metadata": {},
   "source": [
    "# import os\n",
    "\n",
    "def load_cache(path):\n",
    "    if path.exists():\n",
    "        return pd.read_csv(path).set_index(\"query\").T.to_dict()\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache, path):\n",
    "    rows = [{\"query\": q, **v} for q,v in cache.items()]\n",
    "    pd.DataFrame(rows).to_csv(path, index=False)\n",
    "\n",
    "cache = load_cache(CACHE_CSV)\n",
    "\n",
    "unique_queries = sorted(set(df[\"_query\"]))\n",
    "QPS = 5\n",
    "sleep_interval = 1.0 / QPS\n",
    "\n",
    "for q in tqdm(unique_queries, desc=\"Geocoding\"):\n",
    "    if q in cache and cache[q].get(\"status\") in (\"OK\",\"ZERO_RESULTS\"):\n",
    "        continue\n",
    "    lat, lng, status, fmt = geocode_with_backoff(q)\n",
    "    cache[q] = {\"lat\": lat, \"lng\": lng, \"status\": status, \"formatted_address\": fmt}\n",
    "    time.sleep(sleep_interval)\n",
    "\n",
    "save_cache(cache, CACHE_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8203146d-7bb6-4f2b-ada6-a125e2945c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "增强完成！结果保存到： /Users/xushixing/Desktop/Comp5339 Assignment 1/stations_with_geo.csv\n"
     ]
    }
   ],
   "source": [
    "df[\"latitude\"] = df[\"_query\"].map(lambda q: cache.get(q, {}).get(\"lat\"))\n",
    "df[\"longitude\"] = df[\"_query\"].map(lambda q: cache.get(q, {}).get(\"lng\"))\n",
    "df[\"geocode_status\"] = df[\"_query\"].map(lambda q: cache.get(q, {}).get(\"status\"))\n",
    "df[\"formatted_address\"] = df[\"_query\"].map(lambda q: cache.get(q, {}).get(\"formatted_address\"))\n",
    "\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(\"增强完成！结果保存到：\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c05f3d-bf04-4227-a46a-3ed1e8b8e385",
   "metadata": {},
   "source": [
    "## store into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17b55c-b06a-4174-860a-f7cf015b5b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a779704-8cb7-4a45-8d46-6721e2fcd5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1) Connect to PostgreSQL ==========\n",
    "url = URL.create(\n",
    "    \"postgresql+psycopg2\",\n",
    "    username=\"postgres\",   # database username11111\n",
    "    password=\"0823\",       # database password\n",
    "    host=\"localhost\",      # host\n",
    "    port=5433,             # port\n",
    "    database=\"5339\",       # database name\n",
    ")\n",
    "engine = create_engine(url, future=True)\n",
    "\n",
    "# ========== 2) Read and normalize stations_with_geo.csv ==========\n",
    "fac = pd.read_csv(\"stations_with_geo.csv\")\n",
    "\n",
    "# Automatically detect longitude/latitude columns (case-insensitive, handles spaces and common aliases)\n",
    "def find_col(cols, patterns):\n",
    "    cols_lower = {c.lower(): c for c in cols}\n",
    "    for pat in patterns:\n",
    "        # First try exact match\n",
    "        if pat in cols_lower:\n",
    "            return cols_lower[pat]\n",
    "        # Then try regex fuzzy match\n",
    "        rx = re.compile(pat, re.I)\n",
    "        for c in cols:\n",
    "            if rx.fullmatch(c) or rx.search(c):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "lon_candidates = [\n",
    "    \"lon\", \"longitude\", r\"long(it(u|)de)?\", r\"\\blng\\b\", r\"^x$\", r\"^long$\", r\"^longtitude$\"\n",
    "]\n",
    "lat_candidates = [\n",
    "    \"lat\", \"latitude\", r\"lat(it(u|)de)?\", r\"^y$\"\n",
    "]\n",
    "\n",
    "lon_col = find_col(fac.columns, lon_candidates)\n",
    "lat_col = find_col(fac.columns, lat_candidates)\n",
    "\n",
    "# Standardize column names to lon / lat\n",
    "if lon_col != \"lon\":\n",
    "    fac = fac.rename(columns={lon_col: \"lon\"})\n",
    "if lat_col != \"lat\":\n",
    "    fac = fac.rename(columns={lat_col: \"lat\"})\n",
    "\n",
    "# Force longitude/latitude to numeric\n",
    "fac[\"lon\"] = pd.to_numeric(fac[\"lon\"], errors=\"coerce\")\n",
    "fac[\"lat\"] = pd.to_numeric(fac[\"lat\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dd98d43-6407-44ef-bbab-c53a52a9a6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded into PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# ========== 3) Write into dim_facility table ==========\n",
    "fac.to_sql(\"dim_facility\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "# ========== 4) Import ABS_NGER_CER_merged.csv ==========\n",
    "fact = pd.read_csv(\"ABS_NGER_CER_merged.csv\")\n",
    "fact.to_sql(\"fact_state_year\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "# ========== 5) Create PostGIS geometry column and populate ==========\n",
    "with engine.begin() as conn:\n",
    "    # Ensure PostGIS is available (you already installed it, this is just safe-guard)\n",
    "    conn.exec_driver_sql(\"CREATE EXTENSION IF NOT EXISTS postgis;\")\n",
    "    # Add geometry column (Point type, SRID = 4326)\n",
    "    conn.exec_driver_sql(\"\"\"\n",
    "        ALTER TABLE dim_facility\n",
    "        ADD COLUMN IF NOT EXISTS geom geometry(Point,4326);\n",
    "    \"\"\")\n",
    "    # Populate geom column using lon/lat\n",
    "    conn.exec_driver_sql(\"\"\"\n",
    "        UPDATE dim_facility\n",
    "        SET geom = ST_SetSRID(ST_MakePoint(lon::double precision, lat::double precision),4326)\n",
    "        WHERE lon IS NOT NULL AND lat IS NOT NULL;\n",
    "    \"\"\")\n",
    "\n",
    "print(\"Successfully loaded into PostgreSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a6a74-750f-437f-a23c-85ad8034e300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
